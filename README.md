# MCP-Flow

**Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools**

> ğŸ“„ [Paper Url](https://arxiv.org/abs/2510.24284)


## ğŸŒ¿ åˆ†æ”¯è¯´æ˜ (Branch Guide)

æœ¬é¡¹ç›®åŒ…å«ä¸¤ä¸ªä¸»è¦åˆ†æ”¯,é€‚ç”¨äºä¸åŒç”¨é€”:

### ğŸ¯ demo åˆ†æ”¯ (æ¨èç”¨äºå¿«é€Ÿæ¼”ç¤º)
- **ç”¨é€”**: å±•ç¤ºé¡¹ç›®æ ¸å¿ƒç®—æ³•å’Œæœ€ç»ˆæˆæœ
- **ç‰¹ç‚¹**:
  - åŒ…å« 80 ä¸ªé«˜è´¨é‡æµ‹è¯•æ ·æœ¬ (100% é€šè¿‡ç‡, è¯„åˆ† 7-10)
  - ä¿ç•™æˆåŠŸçš„ `test_filtration_doubao.py` è„šæœ¬
  - å·²æ¸…ç†å¤±è´¥çš„æµ‹è¯•æ–‡ä»¶å’Œä¸´æ—¶æ•°æ®
  - é€‚åˆå±•ç¤ºã€æ•™å­¦å’Œå¿«é€ŸéªŒè¯
- **åˆ‡æ¢å‘½ä»¤**: `git checkout demo`
- **æ•°æ®ä½ç½®**: `data/test_output/test_filtered_data_doubao.json`

### ğŸ”§ full-pipeline åˆ†æ”¯ (ç”¨äºå®Œæ•´æµç¨‹å¤ç°)
- **ç”¨é€”**: è¿è¡Œå®Œæ•´çš„ 1-4 æ­¥ pipeline
- **ç‰¹ç‚¹**:
  - åŒ…å«å®Œæ•´çš„ `src/main.py` å’Œæ‰€æœ‰æ¨¡å—
  - æ”¯æŒ Server Collection â†’ Tool Extraction â†’ Data Generation â†’ Data Filtration
  - åŒ…å« `requirements.txt` å’Œæ‰€æœ‰ä¾èµ–
  - é€‚åˆå®Œæ•´å¤ç°ã€äºŒæ¬¡å¼€å‘å’Œæ‰©å±•ç ”ç©¶
- **åˆ‡æ¢å‘½ä»¤**: `git checkout full-pipeline`
- **è¿è¡Œæ–¹å¼**:
  ```bash
  # 1. å®‰è£…ä¾èµ–
  pip install -r requirements.txt

  # 2. é…ç½® config.yaml (å¤åˆ¶ config.yaml.example)
  cp config.yaml.example config.yaml
  # ç¼–è¾‘ config.yaml å¡«å…¥ä½ çš„ API å¯†é’¥

  # 3. è¿è¡Œå®Œæ•´ pipeline
  python src/main.py

  # æˆ–åˆ†æ­¥è¿è¡Œ
  python src/main.py --step collect    # æ­¥éª¤1: æ”¶é›†æœåŠ¡å™¨
  python src/main.py --step generate   # æ­¥éª¤2-3: æå–å·¥å…·+ç”Ÿæˆæ•°æ®
  python src/main.py --step filter     # æ­¥éª¤4: è´¨é‡è¿‡æ»¤
  ```


## ğŸ—“ï¸ News

* **ğŸ§  Oct 28, 2025** â€” MCP-Flow is released on [**arXiv**](https://arxiv.org/abs/2510.24284).
* **ğŸ› ï¸ Nov 10, 2025** â€” We open-source all **server configurations** and **tool information**!


## ğŸ“ Introduction

**MCP-Flow** is an automated **web-agent-driven pipeline** for large-scale **server discovery**, **data synthesis**, and **model training** in the **Model Context Protocol (MCP)** ecosystem.

### ğŸŒ Key Features

* ğŸ¤– **Automated server collection** from *6 major MCP marketplaces*

  <p align="center">
    <img src="assets/mcp-flow.png" alt="Server collection" width="600"/>
  </p>

* ğŸ“Š **Extensive tool coverage:** 1,166 real-world servers, 11,536 tools, and 68K+ instructionâ€“function call pairs

  <p align="center">
    <img src="assets/tool_server.png" alt="Tool coverage" width="600"/>
  </p>

* ğŸ§© **Scale & diversity** far beyond previous benchmarks

  <p align="center">
    <img src="assets/bench.png" alt="Benchmark scale" width="600"/>
  </p>




## ğŸ“‚ Datasets

| Category                         | Path                                           | Description                                                 |
| -------------------------------- | ---------------------------------------------- | ----------------------------------------------------------- |
| ğŸ§  Function calls & trajectories | `./data/function_call/` & `./data/trajectory/` | Example data; full datasets are released on **HuggingFace** |
| âš™ï¸ MCP configurations            | `./data/mcp_config/`                           | Configuration files for discovered servers                  |
| ğŸ§° Tool information              | `./data/tools/`                                | Tool descriptions and schema definitions                    |
| ğŸ’» Source code                   | `./src/`                                       | Core scripts for server deployment    |



## ğŸ› ï¸ Installation

```bash
git clone https://github.com/zhangxiaoyao391/MCP-Flow.git
cd MCP-Flow
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. Configuration

Copy the configuration template and fill in your LLM API credentials:

```bash
cp config.yaml.example config.yaml
```

Edit `config.yaml` to add your API keys:

```yaml
llm_providers:
  primary:
    api_key: "YOUR_API_KEY"
    base_url: "YOUR_BASE_URL"
    model: "YOUR_MODEL_NAME"
```

### 2. Run the Pipeline

#### Full Pipeline (Recommended)
```bash
python src/main.py
```

#### Step-by-Step Execution
```bash
# Step 1: Collect MCP servers and tools
python src/main.py --step collect

# Step 2: Generate function calling data
python src/main.py --step generate

# Step 3: Quality filtering
python src/main.py --step filter
```

#### Test Pipeline (Quick validation with 80 samples)
```bash
python test_generation.py          # Generate 80 test samples
python test_filtration_doubao.py   # Filter with quality threshold
```

### 3. View Results

Generated data will be stored in:
- `data/function_call/` - Function calling examples
- `data/trajectory/` - Complete reasoning trajectories
- `data/filtered/` - High-quality filtered data
- `data/test_output/` - Test outputs

## âš™ï¸ Configuration

### LLM Provider Setup

MCP-Flow supports multiple LLM providers with automatic fallback:

```yaml
llm_providers:
  primary:                    # Primary service (prioritized)
    name: "doubao"
    provider: "openai"        # OpenAI-compatible API
    api_key: "YOUR_KEY"
    base_url: "YOUR_URL"
    model: "Doubao-Seed-1.6"

  fallback:                   # Fallback service (if primary fails)
    name: "openai"
    api_key: "YOUR_KEY"
    model: "gpt-4o-mini"
```

### Data Generation Parameters

```yaml
data_generation:
  instruction_per_tool: 5     # Generate 5 instructions per tool
  evolution_depth: 1          # WizardLM evolution depth (1-3, recommended: 1)
  temperature: 0.7            # Generation temperature
  max_tokens: 4096            # Max tokens
```

### Quality Filtering

```yaml
data_filtering:
  similarity_threshold: 0.8   # Embedding similarity threshold
  quality_score_threshold: 6  # LLM quality score threshold (0-10)
```

## ğŸ“Š Test Results

Our test run with **80 samples** achieved:

| Metric | Result |
|--------|--------|
| **Generated Samples** | 80 |
| **Passed Filter** | 80 |
| **Pass Rate** | 100% |
| **Avg Quality Score** | 9.6/10 |
| **Score Range** | 7-10 |

### Sample Output

```json
{
  "server_name": "Weather Server",
  "tool_name": "get_current_weather",
  "instruction_original": "What's the weather in Tokyo?",
  "instruction_evolved": "Can you provide real-time weather including temperature (Â°C), humidity (%), and wind speed (km/h) for Tokyo's Chiyoda Ward?",
  "function_call": {
    "name": "get_current_weather",
    "arguments": {
      "location": "Tokyo's Chiyoda Ward",
      "units": "celsius"
    }
  }
}
```

## ğŸ“ Project Structure

```
MCP-Flow/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                     # Main entry point
â”‚   â”œâ”€â”€ server_collector/           # Server collection module
â”‚   â”œâ”€â”€ tool_extractor/             # Tool extraction module
â”‚   â”œâ”€â”€ instruction_generator/      # Instruction generation module
â”‚   â”œâ”€â”€ data_filter/                # Data filtering module
â”‚   â””â”€â”€ utils/                      # Utility functions
â”œâ”€â”€ mcp_config/                     # MCP server configs (from marketplaces)
â”‚   â”œâ”€â”€ smithery/                   # Smithery marketplace configs
â”‚   â””â”€â”€ glama/                      # Glama marketplace configs
â”œâ”€â”€ data/                           # Generated data
â”‚   â”œâ”€â”€ servers/                    # Server definitions
â”‚   â”œâ”€â”€ tools/                      # Tool definitions
â”‚   â”œâ”€â”€ function_call/              # Function call data
â”‚   â”œâ”€â”€ trajectory/                 # Complete reasoning trajectories
â”‚   â”œâ”€â”€ filtered/                   # High-quality filtered data
â”‚   â””â”€â”€ test_output/                # Test outputs
â”œâ”€â”€ config.yaml.example             # Configuration template
â”œâ”€â”€ test_generation.py              # Test data generation script
â”œâ”€â”€ test_filtration_doubao.py       # Test filtration script
â””â”€â”€ README.md                       # Project documentation
```

## ğŸ¯ Use Cases

1. **Function Calling Model Training**
   - Prepare high-quality function calling datasets for LLM fine-tuning
   - Support various tool types and calling scenarios

2. **Agent Behavior Research**
   - Study model tool-use capabilities
   - Analyze function calling reasoning processes

3. **Benchmark Construction**
   - Build function calling capability benchmarks
   - Diverse tool and scenario coverage

4. **Prompt Engineering**
   - Learn high-quality function calling prompt patterns
   - Extract best practices

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit Issues and Pull Requests.

1. Fork the project
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request


## ğŸ§¾ Citation

If you find **MCP-Flow** useful in your research, please consider citing:
```bibtex
@misc{wang2025mcpflowfacilitatingllmagents,
      title={MCP-Flow: Facilitating LLM Agents to Master Real-World, Diverse and Scaling MCP Tools},
      author={Wenhao Wang and Peizhi Niu and Zhao Xu and Zhaoyu Chen and Jian Du and Yaxin Du and Xianghe Pang and Keduan Huang and Yanfeng Wang and Qiang Yan and Siheng Chen},
      year={2025},
      eprint={2510.24284},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.24284},
}
```


## ğŸ“§ Contact
If you have any questions or encounter issues, feel free to open an issue or reach out to the authors directly:

ğŸ“® Email: 12321254@zju.edu.cn
ğŸ’¬ WeChat: <br> <img src="assets/wechat.png" alt="WeChat QR" width="80"/>
