# LoRAå¾®è°ƒæ–¹æ¡ˆ - å¿«é€Ÿå‚è€ƒ

## ğŸ“ é¡¹ç›®ç»“æ„

```
mcp-flow-simple/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ convert_to_llama_factory.py    # æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬
â”œâ”€â”€ train_configs/
â”‚   â”œâ”€â”€ lora_config.yaml               # LoRAè®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ ds_z0_config.json              # DeepSpeed ZeRO-0é…ç½®
â”‚   â””â”€â”€ dataset_info.json              # æ•°æ®é›†é…ç½®
â”œâ”€â”€ data/
â”‚   â””â”€â”€ llama_factory/                 # è½¬æ¢åçš„è®­ç»ƒæ•°æ®
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ mcp_lora/                      # è®­ç»ƒè¾“å‡ºç›®å½•
â”œâ”€â”€ train_lora.py                      # Pythonè®­ç»ƒè„šæœ¬ (Windows)
â”œâ”€â”€ train_lora.sh                      # Bashè®­ç»ƒè„šæœ¬ (Linux/Mac)
â”œâ”€â”€ requirements_lora.txt              # LoRAè®­ç»ƒä¾èµ–
â””â”€â”€ LORA_TRAINING_GUIDE.md            # å®Œæ•´ä½¿ç”¨æŒ‡å—
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ (3æ­¥)

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements_lora.txt
```

### 2. è®¾ç½®æ¨¡å‹è·¯å¾„
```bash
# Windows PowerShell
$env:MODEL_PATH="D:\Models\Qwen2.5-7B-Instruct"

# Linux/Mac
export MODEL_PATH="/path/to/model"
```

### 3. å¯åŠ¨è®­ç»ƒ
```bash
# Windows
python train_lora.py

# Linux/Mac
./train_lora.sh
```

## ğŸ“Š è®­ç»ƒé…ç½® (è®ºæ–‡æ ‡å‡†)

| å‚æ•° | å€¼ |
|------|-----|
| LoRA Rank | 64 |
| LoRA Alpha | 128 |
| Learning Rate | 2e-4 |
| Batch Size | 128 |
| Epochs | 1 |
| Optimizer | AdamW |
| DeepSpeed | ZeRO-0 |

## ğŸ’¡ æ˜¾å­˜éœ€æ±‚

| æ¨¡å‹ | å…¨é‡å¾®è°ƒ | LoRAå¾®è°ƒ | èŠ‚çœ |
|------|---------|---------|------|
| 7B | ~40GB | ~16GB | 60% |
| 14B | ~80GB | ~32GB | 60% |

## ğŸ“– è¯¦ç»†æ–‡æ¡£

è¯·æŸ¥çœ‹å®Œæ•´æŒ‡å—: [LORA_TRAINING_GUIDE.md](LORA_TRAINING_GUIDE.md)
